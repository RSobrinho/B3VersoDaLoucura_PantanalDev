{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalar e importar as bibliotecas necessárias\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6Fbtsf3E4ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datefinder"
      ],
      "metadata": {
        "id": "iIin54zWG9_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROBgUQEoECGE"
      },
      "outputs": [],
      "source": [
        "from urllib.request import Request, urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import random\n",
        "import requests\n",
        "import html5lib\n",
        "import os\n",
        "import datefinder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Scraping Inicial das noticias no google\n",
        "\n",
        "\n",
        "WebScrapying das noticias via google news, e gera como saída arquivo(s) csv.\n",
        "\n",
        "### Especificação da pesquisa:\n",
        "- Busca feita no google news\n",
        "- Pesquisas feitas: \"b3\", \"b3 investidor\", \"b3 bolsa de valores\", \"b3 cotacao\", \"b3 acoes\".\n",
        "- Noticias pegas a partir do dia 01/01/2017\n",
        "- 100 noticias por página\n",
        "- Máximo até a pagina 12, ou até a noticia 1100.\n",
        "\n",
        "### Especificação da saída:\n",
        "- 500 noticias por arquivo csv\n",
        "- Divisão de colunas de cada arquivo: title, description, date e link.\n",
        "- Nome do arquivo de saída: \"pantanalX.csv\"\n",
        "\n",
        "***Código retirado de: \"webscraping/pantanal.py\"***\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "uEAMuZERFfCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# root = \"https://google.com/\"\n",
        "archive = \"pantanal\"\n",
        "dup = []\n",
        "# query = [\"b3\"]\n",
        "query = [\"b3\", \"b3 investidor\", \"b3 bolsa de valores\", \"b3 cotacao\", \"b3 acoes\"] # Lista de pesquisas - Não aceita caracteres especiais\n",
        "num = 100 # Número de noticias por página\n",
        "limit = 500 # Limite de noticias por arquivo gerado\n",
        "count = 0 # Contador de noticias armazenadas\n",
        "pages = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100]\n",
        "\n",
        "# Verifica se arquivo csv existe e apaga\n",
        "# if(os.path.isfile(archive)):\n",
        "#     os.remove(archive)\n",
        "\n",
        "# Realiza um for atráves das pages\n",
        "for page in pages : \n",
        "\n",
        "    # Realiza um for atráves das query - implementado desse jeito para deixar mais aleatório\n",
        "    for q in query:\n",
        "        \n",
        "        # Links utilizado para pesquisa, onde o último foi o que trouxe mais noticias \n",
        "        # link = \"https://www.google.com/search?q=\"+str(q).replace(\" \", \"%20\")+\"&tbm=nws&sxsrf=APwXEdd4WplW1U866Oi0W3zXpLoNlF_Imw:1681353590194&source=lnt&tbs=qdr:d&sa=X&ved=2ahUKEwjByvCb6qX-AhW0s5UCHfApAZYQpwV6BAgBEBw&biw=1536&bih=754&dpr=1.25\" # Ultimas 24 horas\n",
        "        # link = \"https://www.google.com/search?q=\"+str(q).replace(\" \", \"%20\")+\"&tbm=nws&sxsrf=APwXEddRj9CC10mMEFKcZlR_1M1bVyhWOA:1682186293598&source=lnt&tbs=qdr:m&sa=X&ved=2ahUKEwi_g-ejiL7-AhU5jZUCHY-DCQ8QpwV6BAgCEB4&biw=1536&bih=754&dpr=1.25\" # Utilmo mês\n",
        "        # link = \"https://www.google.com/search?q=\"+str(q).replace(\" \", \"%20\")+\"&tbas=0&tbm=nws&sxsrf=APwXEdfxSxbkkummzGM1RrPKF9b6HrsMEQ:1682197753156&source=lnt&tbs=qdr:y&sa=X&ved=2ahUKEwjJ9ZL8sr7-AhUGpJUCHZrbBIQQpwV6BAgBEB8&biw=1536&bih=754&dpr=1.25\" # Ultimo ano\n",
        "        # link = \"https://www.google.com/search?q=\"+str(q).replace(\" \", \"%20\")+\"&tbas=0&biw=1536&bih=754&sxsrf=APwXEdfKazbqFtkiPp2Z_OFnimbdc2j0dA%3A1682197756841&source=lnt&tbs=cdr%3A1%2Ccd_min%3A1%2F1%2F2020%2Ccd_max%3A&tbm=nws\" # Desde 2020\n",
        "        link = \"https://www.google.com/search?q=\"+str(q).replace(\" \", \"%20\")+\"&tbas=0&tbs=cdr:1,cd_min:1/1/2017,lr:lang_1pt&tbm=nws&sxsrf=APwXEdclVuQrgls4z9tt-x0-JUeU9kfrYw:1682198361057&source=lnt&lr=lang_pt&sa=X&ved=2ahUKEwipooKetb7-AhUMrJUCHckHDkoQpwV6BAgBEBY&biw=1536&bih=754&dpr=1.25\" # Desde 2017 Somente em Português\n",
        "\n",
        "        # Preparando o Request\n",
        "        headers = {'User-Agent': \"Mozilla/5.0\"}\n",
        "        newLink = link+\"&num=\"+str(num)+\"&start=\"+str(page)\n",
        "        req = Request(newLink, headers={'User-Agent': \"Mozilla/5.0\"})\n",
        "        webpage = urlopen(req).read()\n",
        "\n",
        "        # Exibindo link da pesquisa feita\n",
        "        print(newLink)\n",
        "        \n",
        "        # Acessando conteúdo da página pesquisada\n",
        "        with requests.Session() as c:\n",
        "            soup = BeautifulSoup(webpage, 'html5lib')\n",
        "            \n",
        "            # For para cada item encontrado\n",
        "            for item in soup.find_all('div', attrs={\"class\": \"Gx5Zad\"}):\n",
        "\n",
        "                # Verifica se existe um link com https em sua estrutura\n",
        "                try:\n",
        "                    linkExists = (item.find(\"a\", href=True)['href']).index(\"https\")\n",
        "                except:\n",
        "                    linkExists = False\n",
        "\n",
        "                # Verifica a existencia do link e partir disso pega os outros campos\n",
        "                if(item.find(\"a\", href=True) and linkExists):\n",
        "                    \n",
        "                    title = (item.find(\"div\", attrs={\"class\": \"BNeawe vvjwJb AP7Wnd\"}).get_text())\n",
        "                    desc = (item.find(\"div\", attrs={\"class\": \"BNeawe s3v9rd AP7Wnd\"}).get_text())\n",
        "                    date = (item.find(\"span\", attrs={\"class\": \"r0bn4c rQMQod\"}).get_text())\n",
        "                    raw_link = (item.find(\"a\", href=True)['href']).split(\"/url?q=\")[1].split(\"&sa=U&\")[0]    \n",
        "\n",
        "                    # Troca toda ocorrencia de \";\" para \",\" para evitar possiveis problemas de formatação no csv \n",
        "                    title = title.replace(\";\", \",\")\n",
        "                    desc = desc.replace(\";\", \",\")\n",
        "\n",
        "                    # Verifica se existe uma duplicata\n",
        "                    try:\n",
        "                        dupExists = dup.index(title)\n",
        "                    except:\n",
        "                        dupExists = False\n",
        "\n",
        "                    # Verifica a existencia de todos os campos pegos, se não existe duplicate e grava no arquivo csv\n",
        "                    if(title != None and desc != None and date != None and raw_link != None and dupExists == False):\n",
        "                        \n",
        "                        count += 1\n",
        "                        block = str((count // limit) + 1) # Número do arquivo - para organização\n",
        "                        nameArch = archive+block+\".csv\"\n",
        "                        # print(count)\n",
        "\n",
        "                        # Apenas para deixar descrito o que é cada coluna no arquivo\n",
        "                        if(not os.path.isfile(nameArch)):\n",
        "                            document = open(nameArch, \"a\")\n",
        "                            document.write(\"{}; {}; {}; {} \\n\".format(\"title\", \"description\", \"date\", \"link\"))\n",
        "                            document.close()    \n",
        "\n",
        "                        # Adiciona na lista de possiveis duplicatas\n",
        "                        dup.append(title)\n",
        "\n",
        "                        # Salva a noticia no arquivo\n",
        "                        document = open(nameArch, \"a\", errors='ignore', encoding = \"ISO-8859-1\")\n",
        "                        document.write(\"{}; {}; {}; {} \\n\".format(title, desc, date, raw_link))\n",
        "                        document.close()        \n"
      ],
      "metadata": {
        "id": "kR0QdZ9tEXVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Scraping Detalhado das noticias\n",
        "WebScrapying de cada noticia encontrada no google news, onde é gerado como saída arquivo(s) csv.\n",
        "\n",
        "### Especificação da pesquisa:\n",
        "- É **necessário** que os arquivos csv com as pesquisas estejam na mesma pasta do código \n",
        "- Busca feita por cada noticia encontrada\n",
        "- Cada pesquisa busca a tag `<p>` para encontrar o conteúdo\n",
        "- A data é filtrada do conteúdo encontrado, caso não acha é utilizado a data da busca \n",
        "\n",
        "### Especificação da saída:\n",
        "- 500 noticias por arquivo csv\n",
        "- Divisão de colunas de cada arquivo: title, date, link, conteúdo e feeling (sentimento) da noticia - esse último campo não é preenchido, pois será usado para rotulação mais tarde.\n",
        "- Nome do arquivo: \"blocoX.csv\"\n",
        "\n",
        "***Código retirado de: \"webscraping/datasets.py\"***\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "AFHWQHjnGtRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para leitura dos arquivos\n",
        "def readFile(path):\n",
        "    f = open(path, \"r\", errors='ignore', encoding = \"ISO-8859-1\")\n",
        "    archive = f.read()\n",
        "    f.close()\n",
        "    return archive"
      ],
      "metadata": {
        "id": "1TXWWZDBLkCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dup = [] # Lista de duplicatas\n",
        "archive = \"bloco\" # Nome do arquivo\n",
        "maxArch = 5 # Máximo de arquivos verificados\n",
        "limit = 500 # Limite de noticias por arquivo gerado\n",
        "count = 0 # Contador de noticias armazenadas\n",
        "\n",
        "path = [] # Lista com todas as noticias encontradas\n",
        "\n",
        "# Pega dados dos arquivos\n",
        "for i in range(0, maxArch):\n",
        "    nameArch = \"pantanal\"+str(i+1)+\".csv\"\n",
        "    if(os.path.isfile(nameArch)):\n",
        "        file = readFile(nameArch).split(\"\\n\")\n",
        "        file = filter(None, file)\n",
        "        file = list(map(lambda el: el.split(\";\"), file))\n",
        "        \n",
        "        if(len(path) == 0):\n",
        "            path.extend(file)\n",
        "        else:\n",
        "            path.extend(file[1:])\n",
        "\n",
        "# Acessa todas as noticias a partir da linha 1\n",
        "for new in path[1:]:\n",
        "    title = new[0] \n",
        "    link = new[-1] \n",
        "    content = \"\"\n",
        "    print(new[0], link)\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Verifica se existe uma duplicata\n",
        "        try:\n",
        "            dupExists = dup.index(title)\n",
        "        except:\n",
        "            dupExists = False\n",
        "        \n",
        "        if(dupExists == False):\n",
        "\n",
        "            # Preparando Request\n",
        "            req = Request(link, headers={'User-Agent': \"Mozilla/5.0\"})\n",
        "            webpage = urlopen(req, timeout=10).read()\n",
        "\n",
        "            # Acessando conteúdo da página pesquisada\n",
        "            with requests.Session() as c:\n",
        "                # Adiciona na lista de possiveis duplicatas\n",
        "                dup.append(title)\n",
        "                \n",
        "                soup = BeautifulSoup(webpage, 'html5lib')\n",
        "                            \n",
        "                # For para cada item encontrado\n",
        "                for item in soup.find_all('p'):\n",
        "                    content += item.get_text()\n",
        "\n",
        "                # Filtro para o titulo, porém não da certo para a tag H1            \n",
        "                # if(soup.find(\"h1\") != None):\n",
        "                #     title = soup.find(\"h1\").get_text()\n",
        "\n",
        "            # Troca toda ocorrencia de \";\" para \",\" para evitar possiveis problemas de formatação no csv \n",
        "            # Remove também toda formatação de espaços ou identações\n",
        "            content = content.replace(\";\", \",\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
        "            \n",
        "            # Procura possiveis data encontrada no content\n",
        "            dt = datefinder.find_dates(content)\n",
        "            dtNew = \"\"    \n",
        "            \n",
        "            # Em caso de não encontrar nada no content\n",
        "            if (content == \"\"):\n",
        "              content = new[1] # Coloca como content a descrição encontrada no google \n",
        "\n",
        "            # Procura e pega a primeira data no content\n",
        "            for data in dt:\n",
        "                if (dtNew == \"\" and datetime.strptime(\"01/01/2020\", \"%d/%m/%Y\") <= data): \n",
        "                    dtNew = data.strftime('%d/%m/%Y %H:%M')\n",
        "                \n",
        "                content = content.replace(data.strftime('%d/%m/%Y'), \"\")\n",
        "                content = content.replace(data.strftime('%Y-%m-%d'), \"\")\n",
        "\n",
        "            # Em caso de não encontrar data no content\n",
        "            if (dtNew == \"\"):\n",
        "                # dtNew = new[2] # Pega formato da data encontrada no google news\n",
        "                dtNew = datetime.now().strftime('%d/%m/%Y 00:00') # Pega data atual como substituta\n",
        "            \n",
        "            # Verifica a existencia de todos os campos pegos e grava no arquivo csv\n",
        "            if(title != None and content != \"\" and link != None):\n",
        "                count += 1\n",
        "                block = str((count // limit) + 1) # Número do arquivo - para organização\n",
        "                nameArch = archive+block+\".csv\"\n",
        "\n",
        "                # Apenas para deixar descrito o que é cada coluna no arquivo\n",
        "                if(not os.path.isfile(nameArch)):\n",
        "                    document = open(nameArch, \"a\")\n",
        "                    document.write(\"{}; {}; {}; {}; {} \\n\".format(\"title\", \"date\", \"link\", \"content\", \"feeling\"))\n",
        "                    document.close()   \n",
        "\n",
        "                # Salva a noticia no  arquivo\n",
        "                document = open(nameArch, \"a\",  errors='ignore')\n",
        "                document.write(\"{}; {}; {}; {}; \\n\".format(title, dtNew, link, content))\n",
        "                document.close() \n",
        "\n",
        "    except Exception as error:\n",
        "        print(error) # Em caso de erro é exibido a mensagem\n"
      ],
      "metadata": {
        "id": "UbCHWbyGHDVd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}